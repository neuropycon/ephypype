{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# 02. Compute ERP\nThis workflow mainly call the ephypype pipeline computing N170 component\nfrom raw data specified by the user. The first Node of the workflow\n(extract_events Node) extract the events from raw data. The events are saved\nin the Node directory.\nIn the ERP_pipeline the raw data are epoched accordingly to events extracted in\nextract_events Node.\nThe evoked datasets are created by averaging the different conditions specified\nin json file. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Annalisa Pascarella <a.pascarella@iac.cnr.it>\n# License: BSD (3-clause)\n\n# sphinx_gallery_thumbnail_number = 1\n\n\nimport os.path as op\nimport json\nimport pprint  # noqa\nimport ephypype\n\nimport nipype.pipeline.engine as pe\nfrom nipype.interfaces.utility import Function\n\nfrom ephypype.nodes import create_iterator, create_datagrabber\nfrom ephypype.pipelines.preproc_meeg import create_pipeline_evoked\nfrom ephypype.datasets import fetch_erpcore_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us fetch the data first. It is around 90 MB download.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "base_path = op.join(op.dirname(ephypype.__file__), '..', 'doc/workshop')\ndata_path = fetch_erpcore_dataset(base_path)\n\n# Read experiment params as json\nparams = json.load(open(\"params.json\"))\n\npprint.pprint({'parameters': params})\nprint(params[\"general\"])\n\ndata_type = params[\"general\"][\"data_type\"]\nsubject_ids = params[\"general\"][\"subject_ids\"]\nNJOBS = params[\"general\"][\"NJOBS\"]\nsession_ids = params[\"general\"][\"session_ids\"]\n\n# ERP params\nERP_str = 'ERP'\npprint.pprint({'ERP': params[ERP_str]})\nevents_id = params[ERP_str]['events_id']\ncondition = params[ERP_str]['condition']\nbaseline = tuple(params[ERP_str]['baseline'])\nevents_file = params[ERP_str]['events_file']\nt_min = params[ERP_str]['tmin']\nt_max = params[ERP_str]['tmax']\n\n\ndef get_events(raw_ica, subject):\n    '''\n    The events are extracted from annotation. The events are saved\n    to the Node directory.\n    We take the ica file from the\n    preprocessing workflow directory, i.e. the cleaned raw data.\n    '''\n    print(subject, raw_ica)\n    import mne\n    import numpy as np\n\n    rename_events = {\n        '201': 'response/correct',\n        '202': 'response/error'\n    }\n    \n    for i in range(1, 180+1):\n        orig_name = f'{i}'\n        \n        if 1 <= i <= 40:\n            new_name = 'stimulus/face/normal'\n        elif 41 <= i <= 80:\n            new_name = 'stimulus/car/normal'\n        elif 101 <= i <= 140:\n            new_name = 'stimulus/face/scrambled'\n        elif 141 <= i <= 180:\n            new_name = 'stimulus/car/scrambled'\n        else:\n            continue\n\n        rename_events[orig_name] = new_name\n\n    raw = mne.io.read_raw_fif(raw_ica, preload=True)\n    events_from_annot, event_dict  = mne.events_from_annotations(raw)\n    \n    faces = list()\n    car = list()\n    for key in event_dict.keys():\n    \n        if rename_events[key] == 'stimulus/car/normal':\n            car.append(event_dict[key])\n        elif rename_events[key] == 'stimulus/face/normal':\n            faces.append(event_dict[key])\n    print(faces)\n    print(car)\n    merged_events = mne.merge_events(events_from_annot, faces, 1)\n    merged_events = mne.merge_events(merged_events, car, 2)\n    print(events_from_annot[:10])\n    print(merged_events[:10])\n    print(np.sum(merged_events[:,2]==1))\n    print(np.sum(merged_events[:,2]==2))\n        \n\n    event_file = raw_ica.replace('.fif', '-eve.fif')\n    mne.write_events(event_file, merged_events)\n\n    return event_file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defining pipeline to compute inverse solution\n Then, we create our workflow and specify the `base_dir` which tells\n nipype the directory in which to store the outputs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# workflow directory within the `base_dir`\nERP_pipeline_name = ERP_str + '_workflow'\n\nmain_workflow = pe.Workflow(name=ERP_pipeline_name)\nmain_workflow.base_dir = data_path\n\n# We create a node to pass input filenames to DataGrabber from nipype\ninfosource = create_iterator(['subject_id', 'session_id'],\n                             [subject_ids, session_ids])\n\n# and a node to grab data. The template_args in this node iterate upon\n# the values in the infosource node\nica_dir = op.join(\n        data_path, 'preprocessing_workflow', 'preproc_eeg_pipeline')\n\n\ntemplate_path = \"_session_id_%s_subject_id_%s/ica/sub-%s_ses-%s_*filt_ica.fif\"\ntemplate_args = [['session_id', 'subject_id', 'subject_id', 'session_id']]\ndatasource = create_datagrabber(ica_dir, template_path, template_args)\n\n\n# We connect the output of infosource node to the one of datasource.\n# So, these two nodes taken together can grab data.\nmain_workflow.connect(infosource, 'subject_id', datasource,  'subject_id')\nmain_workflow.connect(infosource, 'session_id', datasource, 'session_id')\n\n# We define the Node that encapsulates run_events_concatenate function\nextract_events = pe.Node(\n    Function(input_names=['raw_ica', 'subject'],\n             output_names=['event_file'],\n             function=get_events),\n    name='extract_events')\n\nmain_workflow.connect(datasource, 'raw_file',\n                      extract_events, 'raw_ica')\nmain_workflow.connect(infosource, 'subject_id',\n                      extract_events, 'subject')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ephypype creates for us a pipeline to compute evoked data which can be\nconnected to these nodes we created.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ERP_workflow = create_pipeline_evoked(\n        data_path, data_type=data_type, pipeline_name=\"ERP_pipeline\",\n        events_id=events_id, baseline=baseline,\n        condition=condition, t_min=t_min, t_max=t_max)\n\nmain_workflow.connect(infosource, 'subject_id',\n                      ERP_workflow, 'inputnode.sbj_id')\nmain_workflow.connect(datasource, 'raw_file',\n                      ERP_workflow, 'inputnode.raw')\nmain_workflow.connect(extract_events, 'event_file',\n                      ERP_workflow, 'inputnode.events_file')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To do so, we first write the workflow graph (optional)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "main_workflow.write_graph(graph2use='colored')  # colored\n\n# Finally, we are now ready to execute our workflow.\nmain_workflow.config['execution'] = {'remove_unnecessary_outputs': 'false'}\n# Run workflow locally on 1 CPU\nmain_workflow.run(plugin='LegacyMultiProc', plugin_args={'n_procs': NJOBS})"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}