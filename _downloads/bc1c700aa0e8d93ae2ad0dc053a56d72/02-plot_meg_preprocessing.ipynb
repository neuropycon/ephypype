{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# 02. Preprocess MEG data\nThe `preprocessing pipeline <preproc_meeg>` runs the ICA algorithm for an\nautomatic removal of eyes and heart related artefacts.\nA report is automatically generated and can be used to correct and/or fine-tune\nthe correction in each subject.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import modules\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import json\nimport pprint\n\nimport os.path as op\nimport nipype.pipeline.engine as pe\n\nfrom ephypype.nodes import create_iterator, create_datagrabber\nfrom ephypype.pipelines.preproc_meeg import create_pipeline_preproc_meeg  # noqa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define data and variables\nFirst, we read the experiment parameters from a\n:download:`json <https://github.com/neuropycon/ephypype/tree/master/doc/workshop/meg/params.json>`\nfile and print it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "params = json.load(open(\"params.json\"))\npprint.pprint({'parameters': params[\"general\"]})\n\ndata_type = params[\"general\"][\"data_type\"]\nsubject_ids = params[\"general\"][\"subject_ids\"]\nNJOBS = params[\"general\"][\"NJOBS\"]\nsession_ids = params[\"general\"][\"session_ids\"]\n\nif \"data_path\" in params[\"general\"].keys():\n    data_path = params[\"general\"][\"data_path\"]\nelse:\n    data_path = op.expanduser(\"~\")\nprint(\"data_path : %s\" % data_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we read the parameters for preprocessing from the json file and print\nit. In the json file we set : the names of EoG and ECG channels, the\nfilter settings, the downsampling frequency, the number of ICA components\nspecified as a fraction of explained variance (0.999) and a reject\ndictionary to exclude time segments\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pprint.pprint({'preprocessing parameters': params[\"preprocessing\"]})\n\nl_freq = params[\"preprocessing\"]['l_freq']\nh_freq = params[\"preprocessing\"]['h_freq']\nECG_ch_name = params[\"preprocessing\"]['ECG_ch_name']\nEoG_ch_name = params[\"preprocessing\"]['EoG_ch_name']\nvariance = params[\"preprocessing\"]['variance']\nreject = params[\"preprocessing\"]['reject']\ndown_sfreq = params[\"preprocessing\"]['down_sfreq']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create workflow\nNow, we create our workflow and specify the ``base_dir`` which tells\nnipype the directory in which to store the outputs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "preproc_pipeline_name = 'preprocessing_dsamp_workflow'\n\nmain_workflow = pe.Workflow(name=preproc_pipeline_name)\nmain_workflow.base_dir = data_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Specify Nodes\nThen we create an ``infosurce`` node to pass input filenames to\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "infosource = create_iterator(['subject_id', 'session_id'],\n                             [subject_ids, session_ids])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "the ``datasource`` node to grab data. The ``template_args`` in this node\niterate upon the values in the ``infosource`` node.\nWe look for MEG data contained in ``ses-meg/meg`` folder.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "template_path = '%s/ses-meg/meg/*%s*run*%s*sss*.fif'\ntemplate_args = [['subject_id', 'subject_id', 'session_id']]\ndatasource = create_datagrabber(data_path, template_path, template_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n### Preprocessing Node\nEphypype creates for us a pipeline which can be connected to these\nnodes we created. The preprocessing pipeline is implemented by the function\n:func:`~ephypype.pipelines.preproc_meeg.create_pipeline_preproc_meeg`, thus\nto instantiate this pipeline node, we pass our parameters to it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "preproc_workflow = create_pipeline_preproc_meeg(\n    data_path, pipeline_name=\"preproc_meg_dsamp_pipeline\",\n    l_freq=l_freq, h_freq=h_freq,\n    variance=variance, ECG_ch_name=ECG_ch_name, EoG_ch_name=EoG_ch_name,\n    data_type=data_type, down_sfreq=down_sfreq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Connect Nodes\nFinally, we connect the nodes two at a time. First, we connect the two\noutputs (subject_id and session_id) of the ``infosource`` node to the\n``datasource`` node. So, these two nodes taken together can grab data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "main_workflow.connect(infosource, 'subject_id', datasource, 'subject_id')\nmain_workflow.connect(infosource, 'session_id', datasource, 'session_id')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similarly, for the inputnode of the ``preproc_workflow``. Things will become\nclearer in a moment when we plot the graph of the workflow.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "main_workflow.connect(infosource, 'subject_id',\n                      preproc_workflow, 'inputnode.subject_id')\nmain_workflow.connect(datasource, 'raw_file',\n                      preproc_workflow, 'inputnode.raw_file')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To do so, we first write the workflow graph (optional)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "main_workflow.write_graph(graph2use='colored')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run workflow\nNow, we are now ready to execute our workflow.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "main_workflow.config['execution'] = {'remove_unnecessary_outputs': 'false'}\nmain_workflow.run(plugin='LegacyMultiProc', plugin_args={'n_procs': NJOBS})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results\nThe output is the preprocessed data stored in the workflow directory\ndefined by ``base_dir``. Here we find the folder\n``preprocessing_dsamp_workflow`` where all the results of each iteration are\nsorted by nodes. The cleaned data will be used in `plot_events_inverse`.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}