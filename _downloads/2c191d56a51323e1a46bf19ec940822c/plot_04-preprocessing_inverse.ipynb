{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# 04. Preprocess MEG data and compute inverse solution\nHere we combine the previous pipeline (`preprocessing pipeline <preproc_meeg>`,\n`plot_events_inverse`) in a unique workflow.\nThis example shows how to combine the different NeuroPycon pipeline to create\nan analysis workflow. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Annalisa Pascarella <a.pascarella@iac.cnr.it>\n# License: BSD (3-clause)\n\n# sphinx_gallery_thumbnail_number = 1\n\n\nimport json\nimport pprint  # noqa\n\nimport os.path as op\nimport nipype.pipeline.engine as pe\nfrom nipype.interfaces.utility import IdentityInterface, Function\nimport nipype.interfaces.io as nio\n\nfrom ephypype.nodes import create_iterator, create_datagrabber\nfrom ephypype.pipelines.preproc_meeg import create_pipeline_preproc_meeg  # noqa\nfrom ephypype.pipelines.fif_to_inv_sol import create_pipeline_source_reconstruction  # noqa\n\n\n# Read experiment params as json\nparams = json.load(open(\"params.json\"))\npprint.pprint({'parameters': params})\n\ndata_type = params[\"general\"][\"data_type\"]\nsubject_ids = params[\"general\"][\"subject_ids\"]\nNJOBS = params[\"general\"][\"NJOBS\"]\nif \"data_path\" in params[\"general\"].keys():\n    data_path = params[\"general\"][\"data_path\"]\nelse:\n    data_path = op.expanduser(\"~\")\nprint(\"data_path : %s\" % data_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Read the parameters for preprocessing from a json file and print it\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pprint.pprint({'preprocessing parameters': params[\"preprocessing\"]})\n\nl_freq = params[\"preprocessing\"]['l_freq']\nh_freq = params[\"preprocessing\"]['h_freq']\nECG_ch_name = params[\"preprocessing\"]['ECG_ch_name']\nEoG_ch_name = params[\"preprocessing\"]['EoG_ch_name']\nvariance = params[\"preprocessing\"]['variance']\nreject = params[\"preprocessing\"]['reject']\ndown_sfreq = params[\"preprocessing\"]['down_sfreq']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we create our workflow and specify the `base_dir` which tells\nnipype the directory in which to store the outputs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# workflow directory within the `base_dir`\nwf_name = 'preprocessing_full_inverse'\nmain_workflow = pe.Workflow(name=wf_name)\nmain_workflow.base_dir = data_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we create a node to pass input filenames to DataGrabber from nipype\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "infosource = create_iterator(['subject_id'], [subject_ids])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and a node to grab data. The template_args in this node iterate upon\nthe values in the infosource node\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "template_path = '*'\n\nraw_file = '%s/ses-meg/meg_short/*%s*run*sss*.fif'\ntrans_file = '%s/ses-meg/meg_short/%s%s.fif'\nfield_template = dict(raw_file=raw_file, trans_file=trans_file)\n\ntemplate_args = dict(\n    raw_file=[['subject_id', 'subject_id']],\n    trans_file=[['subject_id', 'subject_id', \"-trans\"]])\n\ndatasource = create_datagrabber(data_path, template_path, template_args,\n                                field_template=field_template,\n                                infields=['subject_id'],\n                                outfields=['raw_file', 'trans_file'])\n\nmain_workflow.connect(infosource, 'subject_id', datasource, 'subject_id')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ephypype creates for us a pipeline which can be connected to these\nnodes we created. The preprocessing pipeline is implemented by the function\nephypype.pipelines.preproc_meeg.create_pipeline_preproc_meeg, thus to\ninstantiate this pipeline node, we import it and pass our\nparameters to it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "preproc_workflow = create_pipeline_preproc_meeg(\n    data_path, l_freq=l_freq, h_freq=h_freq,\n    variance=variance, ECG_ch_name=ECG_ch_name, EoG_ch_name=EoG_ch_name,\n    data_type=data_type, down_sfreq=down_sfreq, mapnode=True)\n\nmain_workflow.connect(infosource, 'subject_id',\n                      preproc_workflow, 'inputnode.subject_id')\nmain_workflow.connect(datasource, 'raw_file',\n                      preproc_workflow, 'inputnode.raw_file')\n\n# Source reconstruction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function to extract events from the stimulus channel\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def run_events_concatenate(list_ica_files, subject):\n    '''\n    The events are extracted from stim channel 'STI101'. The events are saved\n    to the Node directory.\n    For each subject, the different run are concatenated in one single raw file\n    and saved in the Node directory. We take the different run from the\n    preprocessing workflow directory, i.e. the cleaned raw data.\n    '''\n\n    print(subject, list_ica_files)\n    import os\n    import mne\n\n    # could be added in a node to come\n    mask = 4096 + 256  # mask for excluding high order bits\n    delay_item = 0.0345\n    min_duration = 0.015\n\n    print(\"processing subject: %s\" % subject)\n\n    raw_list = list()\n    events_list = list()\n    fname_events_files = []\n\n    print(\"  Loading raw data\")\n    for i, run_fname in enumerate(list_ica_files):\n        run = i+1\n\n        raw = mne.io.read_raw_fif(run_fname, preload=True)\n        events = mne.find_events(raw, stim_channel='STI101',\n                                 consecutive='increasing', mask=mask,\n                                 mask_type='not_and',\n                                 min_duration=min_duration)\n\n        print(\"  S %s - R %s\" % (subject, run))\n\n        fname_events = os.path.abspath('run_%02d-eve.fif' % run)\n        mne.write_events(fname_events, events)\n        fname_events_files.append(fname_events)\n\n        delay = int(round(delay_item * raw.info['sfreq']))\n        events[:, 0] = events[:, 0] + delay\n        events_list.append(events)\n\n        raw_list.append(raw)\n\n    raw, events = mne.concatenate_raws(raw_list, events_list=events_list)\n    raw.set_eeg_reference(projection=True)\n    raw_file = os.path.abspath('{}_sss_filt_dsamp_ica-raw.fif'.format(subject))\n    print(raw_file)\n\n    raw.save(raw_file, overwrite=True)\n\n    event_file = os.path.abspath(\n            '{}_sss_filt_dsamp_ica-raw-eve.fif'.format(subject))\n    mne.write_events(event_file, events)\n\n    del raw_list\n    del raw\n\n    return raw_file, event_file, fname_events_files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Group average on source level\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def compute_morph_stc(subject, conditions, cond_files, subjects_dir):\n    import os.path as op\n    import mne\n\n    print(\"processing subject: %s\" % subject)\n\n    # Morph STCs\n    stc_morphed_files = []\n    for k, cond_file in enumerate(cond_files):\n        print(conditions[k])\n        print(cond_file)\n\n        stc = mne.read_source_estimate(cond_file)\n        print(stc)\n\n        morph = mne.compute_source_morph(\n                        stc, subject_from=subject, subject_to='fsaverage',\n                        subjects_dir=subjects_dir)\n        print(morph)\n\n        stc_morphed = morph.apply(stc)\n        print(stc_morphed)\n\n        stc_morphed_file = op.abspath('mne_dSPM_inverse_morph-%s' % (conditions[k]))  # noqa\n        stc_morphed.save(stc_morphed_file)\n\n        stc_morphed_files.append(stc_morphed_file)\n\n    return stc_morphed_files\n\n\ndef show_files(files):\n    print(files)\n    return files\n\n\n# building full inverse pipeline\ndef create_full_inv_pipeline(data_path, params,\n                             pipeline_name=\"full_inv_pipeline\"):\n\n    # inverse parameters\n    pprint.pprint({'inverse parameters': params[\"inverse\"]})\n\n    events_id = params[\"inverse\"]['events_id']\n    condition = params[\"inverse\"]['condition']\n    t_min = params[\"inverse\"]['tmin']\n    t_max = params[\"inverse\"]['tmax']\n    spacing = params[\"inverse\"]['spacing']  # oct-6\n    snr = params[\"inverse\"]['snr']\n    inv_method = params[\"inverse\"]['method']  # dSPM\n    parc = params[\"inverse\"]['parcellation']  # aparc\n\n    subjects_dir = op.join(data_path, params[\"general\"][\"subjects_dir\"])\n\n    # full inverse pipeline\n    inv_pipeline = pe.Workflow(name=pipeline_name)\n    inv_pipeline.base_dir = data_path\n\n    # define the inputs of the pipeline\n    inputnode = pe.Node(\n        IdentityInterface(fields=['list_ica_files', 'subject_id', 'trans_file']),  # noqa\n        name='inputnode')\n\n    # We connect the output of infosource node to the one of datasource.\n    # So, these two nodes taken together can grab data.\n    inv_pipeline.connect(infosource, 'subject_id', datasource,  'subject_id')\n\n    # We define the Node that encapsulates run_events_concatenate function\n    concat_event = pe.Node(\n        Function(input_names=['list_ica_files', 'subject'],\n                 output_names=['raw_file', 'event_file', 'fname_events_files'],\n                 function=run_events_concatenate),\n        name='concat_event')\n\n    # and its connections to the other nodes\n    inv_pipeline.connect(inputnode, 'list_ica_files',\n                         concat_event, 'list_ica_files')\n    inv_pipeline.connect(inputnode, 'subject_id', concat_event, 'subject')\n\n    ###########################################################################\n    # Ephypype creates for us a pipeline to compute inverse solution which can\n    # be connected to these nodes we created.\n    inv_sol_workflow = create_pipeline_source_reconstruction(\n        data_path, subjects_dir, spacing=spacing, inv_method=inv_method,\n        is_epoched=True, is_evoked=True, events_id=events_id, condition=condition,  # noqa\n        t_min=t_min, t_max=t_max, all_src_space=True, parc=parc, snr=snr)\n\n    inv_pipeline.connect(inputnode, 'subject_id',\n                         inv_sol_workflow, 'inputnode.sbj_id')\n    inv_pipeline.connect(concat_event, ('raw_file', show_files),\n                         inv_sol_workflow, 'inputnode.raw')\n    inv_pipeline.connect(concat_event, ('event_file', show_files),\n                         inv_sol_workflow, 'inputnode.events_file')\n\n    inv_pipeline.connect(inputnode, 'trans_file',\n                         inv_sol_workflow, 'inputnode.trans_file')\n\n    conditions = params[\"inverse\"]['new_name_condition']\n\n    # We define the Node that encapsulates compute_morph_stc function\n    morph_stc = pe.Node(\n        Function(input_names=['subject', 'conditions', 'cond_files', 'subjects_dir'],  # noqa\n                output_names=['stc_morphed_files'],\n                function=compute_morph_stc),\n        name=\"morph_stc\")\n\n    # and its connections to the other nodes\n    inv_pipeline.connect(infosource, 'subject_id', morph_stc, 'subject')\n    inv_pipeline.connect(inv_sol_workflow, 'inv_solution.stc_files',\n                         morph_stc, 'cond_files')\n\n    morph_stc.inputs.conditions = conditions\n    morph_stc.inputs.subjects_dir = subjects_dir\n\n    return inv_pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defining pipeline to compute inverse solution\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inv_pipeline = create_full_inv_pipeline(data_path, params)\n\n# and its connections to the other nodes\nmain_workflow.connect(preproc_workflow, 'ica.ica_file',\n                      inv_pipeline, 'inputnode.list_ica_files')\nmain_workflow.connect(infosource, 'subject_id',\n                      inv_pipeline, 'inputnode.subject_id')\nmain_workflow.connect(datasource, 'trans_file',\n                      inv_pipeline, 'inputnode.trans_file')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Workflow graph\nTo do so, we first write the workflow graph (optional)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "main_workflow.write_graph(graph2use='colored')  # colored"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Take a moment to pause and notice how the connections\nhere correspond to how we connected the nodes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt  # noqa\nimg = plt.imread(op.join(data_path, wf_name, 'graph.png'))\nplt.figure(figsize=(7, 7))\nplt.imshow(img)\nplt.axis('off')\n\nmain_workflow.config['execution'] = {'remove_unnecessary_outputs': 'false'}\nmain_workflow.run(plugin='LegacyMultiProc', plugin_args={'n_procs': NJOBS})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we are now ready to execute our workflow.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "main_workflow.config['execution'] = {'remove_unnecessary_outputs': 'false'}\n\n# Run workflow locally on 1 CPU\nmain_workflow.run(plugin='LegacyMultiProc', plugin_args={'n_procs': NJOBS})"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}